{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример построения  двухслойной нейронной сети на numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Построение двухслойной нейронный сети для классификации цветков ириса\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# sklearn здесь только, чтобы разделить выборку на тренировочную и тестовую\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Шаг 1. Определение функций, которые понадобяться для обучения\n",
    "# преобразование массива в бинарный вид результатов\n",
    "def to_one_hot(Y):\n",
    "    n_col = np.amax(Y) + 1\n",
    "    binarized = np.zeros((len(Y), n_col))\n",
    "    for i in range(len(Y)):\n",
    "        binarized[i, Y[i]] = 1.\n",
    "    return binarized\n",
    "\n",
    "# преобразование массива в необходимый вид\n",
    "def from_one_hot(Y):\n",
    "    arr = np.zeros((len(Y), 1))\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        l = layer2[i]\n",
    "        for j in range(len(l)):\n",
    "            if(l[j] == 1):\n",
    "                arr[i] = j+1\n",
    "    return arr\n",
    "\n",
    "# сигмоида и ее производная\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "#    return x * (x>0)\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "#    return 1.0*(x>=0)\n",
    "\n",
    "# нормализация массива\n",
    "def normalize(X, axis=-1, order=2):\n",
    "    l2 = np.atleast_1d(np.linalg.norm(X, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return X / np.expand_dims(l2, axis)\n",
    "\n",
    "# вычисление точности предсказания по вероятности выбора и соответствующей ван-хот-метке этого же набора\n",
    "def chck_acc(ar_prob, ar_lbl):\n",
    "    pred = np.argmax(ar_prob,axis=1)\n",
    "    pred = to_one_hot(pred) #.astype(np.int)\n",
    "    kv,kh = ar_lbl.shape\n",
    "    return 100* sum(sum(pred==ar_lbl))/(kv*kh)\n",
    "\n",
    "def prediction(X_, w0,w1):\n",
    "    layer0 = X_train\n",
    "    layer1 = sigmoid(np.dot(X_,     w0))\n",
    "    y_ = sigmoid(np.dot(layer1, w1))\n",
    "    return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Шаг 2. Подготовка тренировочных данных\n",
    "# получения данных из csv файла. укажите здесь путь к файлу Iris.csv\n",
    "#iris_data = pd.read_csv(\"lesson_source/Iris.csv\")\n",
    "iris_data = pd.read_csv(\"Iris.csv\")\n",
    "\n",
    "# print(iris_data.head()) # расскоментируйте, чтобы посмотреть структуру данных\n",
    "\n",
    "# репрезентация данных в виде графиков\n",
    "#g = sns.pairplot(iris_data.drop(\"Id\", axis=1), hue=\"Species\")\n",
    "# plt.show() # расскоментируйте, чтобы посмотреть\n",
    "\n",
    "# замена текстовых значений на цифровые\n",
    "iris_data['Species'].replace(['Iris-setosa', 'Iris-virginica', 'Iris-versicolor'], [0, 1, 2], inplace=True)\n",
    "\n",
    "# формирование входных данных\n",
    "columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "x = pd.DataFrame(iris_data, columns=columns)\n",
    "x = normalize(x.as_matrix())\n",
    "\n",
    "# формирование выходных данных(результатов)\n",
    "columns = ['Species']\n",
    "y = pd.DataFrame(iris_data, columns=columns)\n",
    "y = y.as_matrix()\n",
    "y = y.flatten()\n",
    "y = to_one_hot(y)\n",
    "\n",
    "# Разделение данных на тренировочные и тестовые\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.12 47.49883774740075\n",
      "100 0.12 80.28990926882247\n",
      "200 0.12 82.96802090278537\n",
      "300 0.12 85.81394659823492\n",
      "400 0.12 88.77218618463749\n",
      "500 0.12 91.16294745295829\n",
      "600 0.12 92.3859123089577\n",
      "700 0.12 93.35027108743017\n",
      "800 0.12 94.21615873337389\n",
      "900 0.12 94.95050260896825\n",
      "1000 0.12 95.55094672512404\n",
      "1100 0.12 96.0358034386732\n",
      "1200 0.12 96.4283148724508\n",
      "1300 0.12 96.74899582850355\n",
      "1400 0.12 97.01397825528754\n",
      "1500 0.12 97.23551812485462\n",
      "1600 0.12 97.42285205482789\n",
      "1700 0.12 97.58295650711352\n",
      "1800 0.12 97.72113806945752\n",
      "1900 0.12 97.84147149601857\n",
      "2000 0.12 97.94711879686993\n",
      "2100 0.12 98.04056042021722\n",
      "2200 0.12 98.12376303822185\n",
      "2300 0.12 98.19830203731307\n",
      "2400 0.12 98.26545167685121\n",
      "2500 0.12 98.32625207931787\n",
      "2600 0.12 98.38155950545742\n",
      "2700 0.12 98.43208446830512\n",
      "2800 0.12 98.47842091667994\n",
      "2900 0.12 98.5210687966603\n",
      "3000 0.12 98.56045165463695\n",
      "3100 0.12 98.59693049169168\n",
      "3200 0.12 98.6308147572723\n",
      "3300 0.12 98.6623711400928\n",
      "3400 0.12 98.69183064828782\n",
      "3500 0.12 98.7193943501331\n",
      "3600 0.12 98.74523805801061\n",
      "3700 0.12 98.76951617263944\n",
      "3800 0.12 98.79236485552816\n",
      "3900 0.12 98.81390466062516\n",
      "4000 0.12 98.83424272804156\n",
      "4100 0.12 98.85347462119785\n",
      "4200 0.12 98.87168587213372\n",
      "4300 0.12 98.88895328680776\n",
      "4400 0.12 98.90534605210803\n",
      "4500 0.12 98.9209266783347\n",
      "4600 0.12 98.93575180460736\n",
      "4700 0.12 98.94987288962393\n",
      "4800 0.12 98.96333680616951\n",
      "4900 0.12 98.97618635453172\n",
      "Точность нейронной сети на тренировочном наборе 98.99%\n",
      "Точность нейронной сети на тренировочном наборе  100.0%\n",
      "Точность нейронной сети на тестовом наборе  90.67%\n"
     ]
    }
   ],
   "source": [
    "### Шаг 3. Обученние нейронной сети\n",
    "# присваевание случайных весов\n",
    "\n",
    "N1 = 5 # 1000 # 5\n",
    "np.random.seed(1) # 11\n",
    "w0 = 2*np.random.random((4, N1)) - 1 # для входного слоя   - 4 входа, 3 выхода\n",
    "w1 = 2*np.random.random((N1, 3)) - 1 # для внутреннего слоя - 5 входов, 3 выхода\n",
    "\n",
    "# скорость обучения (learning rate)\n",
    "n = 0.12#  0.1# 0.12*0.95 -sigm # 0.008*0.9\n",
    "acc_last = 0\n",
    "# массив для ошибок, чтобы потом построить график\n",
    "errors = []\n",
    "\n",
    "# процесс обучения\n",
    "#for i in range(100000):\n",
    "for i in range(5000):\n",
    "\n",
    "    # прямое распространение(feed forward)\n",
    "    layer0 = X_train\n",
    "    layer1 = sigmoid(np.dot(layer0, w0))\n",
    "    layer2 = sigmoid(np.dot(layer1, w1))\n",
    "    # обратное распространение(back propagation) с использованием градиентного спуска\n",
    "    layer2_error = y_train - layer2\n",
    "    layer2_delta = layer2_error * sigmoid_deriv(layer2)\n",
    "    \n",
    "    layer1_error = layer2_delta.dot(w1.T)\n",
    "    layer1_delta = layer1_error * sigmoid_deriv(layer1)\n",
    "    \n",
    "    w1 += layer1.T.dot(layer2_delta) * n\n",
    "    w0 += layer0.T.dot(layer1_delta) * n\n",
    "    \n",
    "    error = np.mean(np.abs(layer2_error))\n",
    "#    error = np.mean(np.sqrt(layer2_error*layer2_error))\n",
    "\n",
    "    errors.append(error)\n",
    "    accuracy = (1 - error) * 100\n",
    "#    accuracy = chck_acc(layer2, y_train)\n",
    "    if i%10==0: \n",
    "        if acc_last >= accuracy: n = n*0.95\n",
    "        acc_last = accuracy\n",
    "    if i%100==0:\n",
    "        print(i,n,accuracy)\n",
    "        \n",
    "\n",
    "### Шаг 4. Демонстрация полученных результатов\n",
    "# черчение диаграммы точности в зависимости от обучения\n",
    "#plt.plot(errors)\n",
    "#plt.xlabel('Обучение')\n",
    "#plt.ylabel('Ошибка')\n",
    "#plt.show() # расскоментируйте, чтобы посмотреть \n",
    "        \n",
    "print(\"Точность нейронной сети на тренировочном наборе \" + str(round(accuracy,2)) + \"%\")\n",
    "#accuracy = chck_acc(layer2, y_train)\n",
    "#print(\"Точность нейронной сети на тренировочном наборе  \" + str(round(accuracy,2)) + \"%\")\n",
    "\n",
    "y_pred = prediction(X_train, w0,w1)\n",
    "accuracy = chck_acc(y_pred, y_train)\n",
    "print(\"Точность нейронной сети на тренировочном наборе  \" + str(round(accuracy,2)) + \"%\")\n",
    "\n",
    "y_pred = prediction(X_test, w0,w1)\n",
    "accuracy = chck_acc(y_pred, y_test)\n",
    "print(\"Точность нейронной сети на тестовом наборе  \" + str(round(accuracy,2)) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание\n",
    "\n",
    "<ol>\n",
    "    <li>Попробуйте видоизменить параметры разобранной на уроке двухслойной нейронной сети таким образом, чтобы улучшить ее точность. Проведите анализ — что приводит к ухудшению точности нейронной сети? Что приводит к увеличению ее точности?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметрами задачи являются:\n",
    "    скорость обучения (n), число эпох, метрика, функция активации, алгоритм минимизации (менять не буду), \n",
    "    критерий качества (accuracy), размер входного слоя (N1)\n",
    "\n",
    "Для экспериментирования сделал несколько модификаций кода: \n",
    "    - для финальной величины качества аккуратность рассчитывал по классической формуле (TP+TN / TP+TN+FP+FN)\n",
    "    - скорость обучения сделал переменной, уменьшающейся по мере приближения к оптимуму\n",
    "    - зафиксировал исходный рандом: np.random.seed(1) \n",
    "\n",
    "Попытка сильного увеличения стартовой скорости обучения приводит к расходимости алгоритма спуска (появляются nan)  \n",
    "Для функции активации sigmoid начальную скорость можно ставить n=0.12, для ReLu n=0.01\n",
    "Функция активации sigmoid более подходящая для задачи, хотя ReLu вначале и сходится быстрее, зато потом останавливается,\n",
    "    не доходя до минимума ошибки. Для ReLu достаточно делать 400 шагов, а для sigmoid нужны - 4000 шагов.\n",
    "    Однако, обучение с большим числом эпох приводит на малоразмерном входном слое к переобучению (tren- 100%, test - 90%)\n",
    "\n",
    "Размер входного слоя 5 оказался оптимальным при меньшем размере качество ответов ухудшается, \n",
    "при большем - скорость минимизации снижается, но итоговая точность (по заданному в программе алгоритму - вероятность) \n",
    "слегка повышается, однако же при всём - количество угаданных сэмплов остается тем же!!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
